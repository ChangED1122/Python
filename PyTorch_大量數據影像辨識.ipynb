{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsi5U+AUIk2HgsW/WBOYHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChangED1122/Python/blob/Python-PyTorch/PyTorch_%E5%A4%A7%E9%87%8F%E6%95%B8%E6%93%9A%E5%BD%B1%E5%83%8F%E8%BE%A8%E8%AD%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ob3qgG2Mqp_J"
      },
      "outputs": [],
      "source": [
        "import requests #發送網址請求\n",
        "\n",
        "url = \"https://firebasestorage.googleapis.com/v0/b/grandmacan-2dae4.appspot.com/o/ML_data%2Fone_piece_full.zip?alt=media&token=937656fd-f5c1-44f5-b174-1e2d590b8ef3\"\n",
        "with open(\"one_piece_full.zip\",\"wb\") as f:  #wb 二進制格式\n",
        "  req = requests.get(url)\n",
        "  f.write(req.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"one_piece_full.zip\",\"r\") as zip_file:\n",
        "  zip_file.extractall(\"one_piece_full\") #解壓縮"
      ],
      "metadata": {
        "id": "yxD7eRT6q2R9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"one_piece_full/classnames.txt\",\"r\") as f :\n",
        "  lines = f.readlines()\n",
        "  classes = [line.strip() for line in lines] #使用strip()將 \\n 移除\n",
        "  print(classes) #取得種類"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kzjh-wETGHZ",
        "outputId": "88790fcb-2388-45a8-b845-7a0895e576d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ace', 'Akainu', 'Brook', 'Chopper', 'Crocodile', 'Franky', 'Jinbei', 'Kurohige', 'Law', 'Luffy', 'Mihawk', 'Nami', 'Rayleigh', 'Robin', 'Sanji', 'Shanks', 'Usopp', 'Zoro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImageDataset(Dataset):  #創建類別，繼承Dataset\n",
        "  def __init__(self, root, train, transform=None): #初始化\n",
        "\n",
        "    if train:\n",
        "      image_root = Path(root) / \"train\"\n",
        "    else :\n",
        "      image_root = Path(root) / \"test\"\n",
        "\n",
        "    with open(Path(root)/ \"classnames.txt\",\"r\") as f :\n",
        "      lines = f.readlines()\n",
        "      self.classes = [line.strip( ) for line in lines] #classes=所有類別的列表，使用strip()將 \\n 移除\n",
        "\n",
        "    self.paths = [i for i in image_root.rglob(\"*\") if i.is_file()]  #所有圖片路徑\n",
        "    self.transform = transform\n",
        "    \n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.paths[index]).convert(\"RGB\")\n",
        "    class_name = self.paths[index].parent.name #取得資料夾名稱（角色名）\n",
        "    class_idx = self.classes.index(class_name) #取得角色之類別索引\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx \n",
        "    else:\n",
        "      return img, class_idx\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.paths) #回傳圖片張數\n",
        "    "
      ],
      "metadata": {
        "id": "Pz22xtLWq4DK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([ #把以上指令結合\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.TrivialAugmentWide(), # 隨機幫圖片做轉換 只要在訓練集做轉換就好\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([ \n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "hoeP3zakq6Io"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(root = \"one_piece_full\",\n",
        "                             train=True, \n",
        "                             transform = train_transforms)\n",
        "test_dataset = ImageDataset(root = \"one_piece_full\",\n",
        "                             train=False, \n",
        "                             transform = test_transforms)\n"
      ],
      "metadata": {
        "id": "1EIT57Geq75z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset),len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgUU3QFlCKgO",
        "outputId": "c3f959a2-de4a-447a-fa83-fdc8928e6431"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3015, 750)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False)"
      ],
      "metadata": {
        "id": "SoiMlFZgBdof"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader), len(test_dataloader) #3016/16, 750/16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BDpYkTHCSVx",
        "outputId": "c2c651a3-02ef-4b80-e3a6-b457d56c1796"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(189, 47)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class ImageClassificationModel3(nn.Module):\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "                out_channels=8, \n",
        "                kernel_size=(3,3), \n",
        "                stride=1, \n",
        "                padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=8,\n",
        "                out_channels=8, \n",
        "                kernel_size=(3,3), \n",
        "                stride=1, \n",
        "                padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=(2,2), \n",
        "                   stride=2, \n",
        "                   padding=0\n",
        "      )\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=8,\n",
        "                out_channels=16, \n",
        "                kernel_size=(3,3), \n",
        "                stride=1, \n",
        "                padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=16,\n",
        "                out_channels=16, \n",
        "                kernel_size=(3,3), \n",
        "                stride=1, \n",
        "                padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=(2,2), \n",
        "                   stride=2, \n",
        "                   padding=0\n",
        "      )\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(start_dim=1,end_dim=-1), #預設是（1,-1）這樣沒有全部攤平\n",
        "        nn.Linear(in_features=16*16*16, out_features=output_shape) #如果不知道in_features要填多少，可以先跑block_1&block_2\n",
        "        #nn.Softmax()  因為torch的 CrossEntropy已經包含 Softmax()\n",
        "    )\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "1X2yTf5aCtJq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "17_xlSK4CxEn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0NBpFbhDGrQ",
        "outputId": "7e2cb241-8468-48d4-b98a-8380b639c44b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stob5NMlHB2H",
        "outputId": "de61568b-a2e9-4bf4-8d7d-2c1b886fffca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = ImageClassificationModel3(3, len(train_dataset.classes)) #input_shape彩色圖片 =3 , output_shape Y的類別\n",
        "model_2.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY_eir1dQ4KM",
        "outputId": "5a2237d0-f2e3-4ec8-f594-2dbfff31b564"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassificationModel3(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=4096, out_features=18, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_2.parameters(), lr=0.001) "
      ],
      "metadata": {
        "id": "SJfngXqjCanz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_pred, y_true):\n",
        "  correct_num = ( y_pred==y_true).sum()\n",
        "  acc = correct_num/len(y_true) *100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "fUBP4LQiC1kc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(dataloader, model, cost_fn, optimizer, accuracy_fn, device): #訓練步驟\n",
        "   train_cost = 0\n",
        "   train_acc = 0\n",
        "\n",
        "   for batch, (x, y) in enumerate(dataloader):  # enumerate( ) 知道第幾個batch\n",
        "    x= x.to(device)\n",
        "    y= y.to(device)\n",
        "    model.train()\n",
        "\n",
        "    y_pred = model(x)\n",
        "\n",
        "    cost = cost_fn(y_pred, y)\n",
        "    train_cost += cost   #  train_cost的值=每個batch的cost加總\n",
        "\n",
        "    train_acc += accuracy_fn(y_pred.argmax(dim=1) ,y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "   train_cost /= len(train_dataloader)  #取平均\n",
        "   train_acc /= len(train_dataloader)\n",
        "   print(f\"\\nTrain Cost: {train_cost:.4f}, Train Acc: {train_acc:.2f}\")\n",
        "\n",
        "def test_step(dataloader, model, cost_fn, accuracy_fn, device): #測試步驟\n",
        "  test_cost = 0\n",
        "  test_acc = 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for x, y in dataloader:\n",
        "      x= x.to(device)\n",
        "      y= y.to(device)\n",
        "      test_pred = model(x)\n",
        "      \n",
        "      test_cost += cost_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(test_pred.argmax(dim=1) ,y)\n",
        "\n",
        "    test_cost /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "  \n",
        "  print(f\"Test  Cost: {test_cost:.4f}, Test  Acc: {test_acc:.2f}  \\n\")"
      ],
      "metadata": {
        "id": "Lvk8SE1iC4ir"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  #進度條\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n------\")\n",
        "  train_step(train_dataloader, model_2, cost_fn, optimizer, accuracy_fn, device)\n",
        "\n",
        "  test_step(test_dataloader, model_2, cost_fn, accuracy_fn, device)"
      ],
      "metadata": {
        "id": "NLPQSFpgClBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "遷移學習 transfer Learning\n",
        "\n",
        "https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights"
      ],
      "metadata": {
        "id": "DRU3paZpoo4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT #取得參數\n",
        "model = torchvision.models.efficientnet_b0(weights=weights)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "pns64J3p1gSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZxFb-Q22g6",
        "outputId": "5f3a47a3-6f27-43de-9f34-d02c0eab7b39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#將模型架構條列出來\n",
        "summary(model = model,\n",
        "        input_size=(16,3,64,64), # 跑完EfficientNet 變成 [16, 1000]\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\" ],\n",
        "        row_settings=[\"var_names\"])  #多了每一層的變數名稱"
      ],
      "metadata": {
        "id": "2C_4eo_33Zus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "調整模型 因借來的模型數出值有1000個類別 ＝> 改成18個值\n",
        "\n",
        "classifier底下需更改. # model.名稱[哪一層]\n",
        "\n",
        "改完回去跑summary 檢查"
      ],
      "metadata": {
        "id": "_BJqUEME6HyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "model.classifier[0] #Dropout層\n",
        "model.classifier[1] = nn.Linear(in_features=1280, out_features=18, bias=True).to(device) # 更改Linear層中out_features=1000\n"
      ],
      "metadata": {
        "id": "9FpiHEbs5Vf4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "除了最後一層（classifier）需要重新訓練以外，上面每層參數鎖住（沒有追蹤梯度） ，訓練的時候參數值就不會變\n",
        "\n",
        "trainable -> False"
      ],
      "metadata": {
        "id": "MsobNuR-78CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in  model.features.parameters():\n",
        "  param.requires_grad=False\n",
        "  print (param)"
      ],
      "metadata": {
        "id": "goZ_sXPt8bmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "了解 EfficientNet 在訓練時須經過什麼樣的轉換\n",
        "\n",
        "我們的資料集也需要相同轉換"
      ],
      "metadata": {
        "id": "OdPLLG5A-TXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT #取得參數\n",
        "EfficientNet_b0_transforms = weights.transforms()"
      ],
      "metadata": {
        "id": "sfFmwVT89_YR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(root=\"one_piece_full\",\n",
        "                             train=True, \n",
        "                             transform=EfficientNet_b0_transforms )\n",
        "test_dataset = ImageDataset(root=\"one_piece_full\",\n",
        "                             train=False, \n",
        "                             transform=EfficientNet_b0_transforms )"
      ],
      "metadata": {
        "id": "OJ4J8KKY_Bx6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False)"
      ],
      "metadata": {
        "id": "XfYGglXz_oyi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader),len(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XtMTfE1_xTP",
        "outputId": "0d7f8e6e-649d-4c8f-df24-dd5e660d8fb5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(189, 47)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "nbnFQTe__5Rz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  #進度條\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n------\")\n",
        "  train_step(train_dataloader, model, cost_fn, optimizer, accuracy_fn, device)\n",
        "\n",
        "  test_step(test_dataloader, model, cost_fn, accuracy_fn, device)"
      ],
      "metadata": {
        "id": "GsxW3yalAKdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"1161514004.jpg\").convert(\"RGB\") #上面怎麼讀取跟轉換這裡一樣\n",
        "img\n",
        "img = EfficientNet_b0_transforms(img)\n",
        "img.shape # [3,224,224]\n",
        "img = img.reshape(-1,3,224,224) \n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  y_pred = model(img.to(device))\n",
        "\n",
        "y_pred = torch.softmax(y_pred, dim=1) #轉換成機率\n",
        "class_idx = y_pred.argmax(dim=1)\n",
        "train_dataset.classes[class_idx]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "0wozE3AbDdsp",
        "outputId": "4f21072b-0be6-4337-c264-719851970174"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zoro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}